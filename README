# xGPU-future: CUDA 12.x Compatible Correlator Library

[![CUDA Version](https://img.shields.io/badge/CUDA-12.x%2B-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Validation](https://img.shields.io/badge/Validation-Bit%20Perfect-brightgreen.svg)](#scientific-validation)

> **Modernized xGPU for CUDA 12.x+ compatibility with texture objects migration**

## Overview

xGPU is a high-performance GPU library for performing the cross-multiplication step of the FX correlator algorithm, essential for radio astronomy signal processing. This modernized version maintains **exact computational equivalence** with the original while supporting current and future CUDA toolkit versions.

### Key Features

- üöÄ **CUDA 12.x+ Compatible**: Migrated from deprecated texture references to modern texture objects
- üî¨ **Scientifically Validated**: Bit-for-bit identical results with comprehensive cross-version testing
- ‚ö° **High Performance**: Optimized GPU kernels achieving teraflop-scale performance
- üß™ **Comprehensive Testing**: Docker-based validation framework with automated comparison
- üìö **Well Documented**: Complete migration guides and testing procedures

### What's New in This Version

‚ú® **CUDA 12.x Modernization** - Complete texture object migration while preserving all functionality  
üîß **Enhanced Testing** - Comprehensive validation framework ensuring migration accuracy  
üìñ **Improved Documentation** - Detailed guides for cross-version validation and deployment  
üê≥ **Docker Support** - Containerized testing for consistent validation across environments  

## Precision and Performance

### Computational Modes

- **Default Mode**: 8-bit integer input ‚Üí 32-bit floating point computation and output
- **DP4A Mode**: Pure integer pipeline (8-bit multiply, 32-bit accumulation) on supported architectures
  - **Supported**: sm_61, sm_70, sm_72, sm_75, sm_80, sm_86, sm_89, sm_90+
  - **Performance**: Up to 4x speedup vs floating point on compatible hardware
  - **Fallback**: Automatic emulation on unsupported architectures (correct but slower)

## Compatibility

### CUDA Toolkit Support
- ‚úÖ **CUDA 12.x+**: Full native support with texture objects

> **Note**: This modernized version requires CUDA 12.x or later. For CUDA 11.x and earlier, use the original xGPU implementation.

### GPU Architecture Support

The modernized xGPU implementation supports CUDA 12+ compatible GPU architectures with compute capability 5.0 or higher.

| Architecture | Compute Capability | Example GPUs | CUDA 12+ Support |
|--------------|-------------------|--------------|-------------------|
| **Maxwell** | 5.0, 5.2 | GTX 750, GTX 980, Tesla M40 | ‚úÖ Full Support |
| **Pascal** | 6.0, 6.1 | Tesla P100, GTX 1080, Tesla P40 | ‚úÖ Full Support |
| **Volta** | 7.0 | Tesla V100, Titan V | ‚úÖ Full Support |
| **Turing** | 7.5 | RTX 2060, RTX 2070, RTX 2080 Ti, Tesla T4 | ‚úÖ Full Support |
| **Ampere** | 8.0, 8.6, 8.7 | RTX 3060, RTX 3070, RTX 3080, RTX 3090, A100 | ‚úÖ Full Support |
| **Ada Lovelace** | 8.9 | RTX 4060, RTX 4070, RTX 4080, RTX 4090 | ‚úÖ Full Support |
| **Hopper** | 9.0 | H100, H200 | ‚úÖ Full Support |
| **Blackwell** | 10.0+ | B100, B200 | ‚úÖ Full Support |

> **Requirements**: CUDA 12.x requires **compute capability 5.0 or higher** (Maxwell and newer). Note that CUDA 13.0+ requires compute capability 7.5+, but this implementation targets CUDA 12.x compatibility.

### DP4A Integer Optimization

Modern GPU architectures support accelerated integer arithmetic via DP4A instructions:

- **Supported**: sm_61, sm_70, sm_75, sm_80, sm_86, sm_89, sm_90+ (Pascal and newer)
- **Performance**: Up to 4x speedup vs floating point computation
- **Fallback**: Automatic emulation on unsupported architectures (correct but slower)

## Quick Start

### Prerequisites
- **CUDA Toolkit**: 12.x or later required
- **GPU**: Compute capability 5.0+ (Maxwell or newer)
- **OS**: Linux (Ubuntu 18.04+, CentOS 7+, RHEL 8+)
- **Compiler**: gcc/g++ with CUDA support

### Basic Build

```bash
# Clone the repository
git clone https://github.com/jbmorgado/xGPU-future.git
cd xGPU-future

# Build with default settings (sm_35, 1D textures)
cd src
make

# Build for specific GPU architecture
make CUDA_ARCH=sm_80  # For A100/RTX 3090

# Build with DP4A optimization (on supported hardware)
make CUDA_ARCH=sm_80 DP4A=yes
```

### Build Configuration Options

**Core Options:**
- `CUDA_ARCH=sm_XX` - Target GPU architecture (default: sm_35)
- `CUDA_DIR=/path` - CUDA toolkit path (default: /usr/local/cuda)
- `TEXTURE_DIM=1|2` - Texture dimension mode (default: 1)
- `DP4A=yes|no` - Enable DP4A optimizations (default: no)

**Sizing Parameters:**
- `NPOL=2` - Number of polarizations (default: 2)
- `NSTATION=256` - Number of stations (default: 256)
- `NFREQUENCY=10` - Number of frequency channels (default: 10)
- `NTIME=1024` - Time samples per integration (default: 1024)
- `NTIME_PIPE=128` - Time samples per GPU transfer (default: 128)

**Advanced Options:**
- `DEBUG=-g` - Debug flags (default: -O3)
- `VERBOSE=1` - Verbose compilation output

### Example Builds

```bash
# High-performance build for modern GPUs
make CUDA_ARCH=sm_80 DP4A=yes TEXTURE_DIM=2

# Custom correlator size
make NSTATION=512 NFREQUENCY=20 NTIME=2048

# Development build with debugging
make DEBUG="-g -DDEBUG" VERBOSE=1
```

## Installation

```bash
cd src

# Install to default location (/usr/local)
sudo make install

# Install to custom location
make install prefix=$HOME/local

# Install with specific architecture
make install CUDA_ARCH=sm_80 prefix=/opt/xgpu
```

**Installed Components:**
- `xgpuinfo` - Library information utility
- `cuda_correlator` - Sample test program  
- `libxgpu.so` - Shared library
- `xgpu.h` - Header file
- `xgpu.m4` - Autoconf macros

## Usage

### C/C++ Integration

```c
#include <xgpu.h>

// Initialize context
XGPUContext context;
xgpuInit(&context, device_id);

// Set up data buffers
// ... populate input arrays ...

// Execute correlation
xgpuCudaXengine(&context, SYNCOP_DUMP);

// Access results
// ... process output matrix ...

// Cleanup
xgpuFree(&context);
```

### Library Information

```bash
# Get library configuration
./xgpuinfo

# Run basic functionality test
./cuda_correlator

# Performance benchmarking
./bench
```

For detailed API documentation, see comments in `src/xgpu.h` and examples in `src/cuda_correlator.c`.

## Testing and Validation

### Simplified Testing

```bash
# Quick local testing
./test-runner.sh basic

# Comprehensive cross-version validation
./test-runner.sh cross-version
```

### Advanced Testing

```bash
# Enter test directory
cd test

# Local testing without Docker
make test-both

# Docker-based validation
./comprehensive-test.sh --full

# Compare specific configurations
./comprehensive-test.sh -c texture_stress
./comprehensive-test.sh -c memory_intensive
```

### Scientific Validation

This modernized version has undergone comprehensive validation:

- ‚úÖ **Bit-Perfect Results**: 0.00e+00 difference ensures complete accuracy
- ‚úÖ **Cross-Version Testing**: Automated Docker-based validation framework
- ‚úÖ **Multiple Configurations**: 8 test scenarios from micro to memory-intensive
- ‚úÖ **Performance Verification**: Zero regression in computational throughput

For detailed testing documentation:
- [`test/README.md`](test/README.md) - Testing framework overview
- [`test/CROSS_VERSION_TESTING.md`](test/CROSS_VERSION_TESTING.md) - Cross-version validation guide
- [`CUDA_MIGRATION_COMPARISON.md`](CUDA_MIGRATION_COMPARISON.md) - Complete technical comparison

## Performance Benchmarking

### CUBE Benchmarking System

xGPU includes CUBE (CUDA BEnchmarking) for comprehensive performance analysis:

```bash
cd src

# Full benchmark suite
./bench

# Custom benchmarking with specific parameters
./bench NSTATION=512 NFREQUENCY=20 CUDA_ARCH=sm_80
```

**Benchmark Metrics:**
- Arithmetic throughput (FLOPS)
- Memory bandwidth utilization
- Kernel execution timing
- Host-device transfer performance

Results are output to `cube_benchmark.log` and `cube_benchmark.csv` for analysis.

## Documentation

### Core Documentation
- **[Migration Comparison](CUDA_MIGRATION_COMPARISON.md)** - Complete technical analysis of CUDA 11‚Üí12 migration
- **[Testing Guide](test/README.md)** - Comprehensive testing framework documentation
- **[Cross-Version Validation](test/CROSS_VERSION_TESTING.md)** - Scientific validation procedures

### API Reference
- **`src/xgpu.h`** - Complete API documentation with function descriptions
- **`src/cuda_correlator.c`** - Example usage and integration patterns

### Build System
- **`src/Makefile`** - Complete build options and environment variables
- **Build guides** - Architecture-specific optimization recommendations

## Citation

If you use this modernized version in your research, please cite the original work:

**Original xGPU:**
```bibtex
@article{clark2013accelerating,
  title={Accelerating Radio Astronomy Cross-Correlation with Graphics Processing Units},
  author={Clark, M. A. and La Plante, P. C. and Greenhill, L. J.},
  journal={arXiv preprint arXiv:1107.4264},
  year={2013}
}
```

## Authors and Contributors

### Original xGPU Development
- **Kate Clark** (NVIDIA) - Original implementation and optimization
- **Paul La Plante** (Loyola University Maryland) - Algorithm development  
- **Lincoln Greenhill** (Harvard-Smithsonian Center for Astrophysics) - Scientific requirements
- **David MacMahon** (University of California, Berkeley) - System integration
- **Ben Barsdell** (NVIDIA) - Performance optimization

### CUDA 12.x Modernization
- **Jorge Bruno Morgado** (Faculty of Science, Porto) - Texture object migration, testing framework, and modernization

## Acknowledgments

The CUDA 12.x modernization was made possible through hardware support provided by **NVIDIA Corporation**, enabling comprehensive testing and validation of the texture object migration across multiple GPU architectures.

## Technical Migration Details

### CUDA 12.x Modernization Summary

This version represents a complete modernization of xGPU for CUDA 12.x+ compatibility:

**üîß Core Changes:**
- **Texture Memory**: Migrated from deprecated texture references to modern texture objects
- **Kernel Interfaces**: Updated function signatures to pass texture objects as parameters  
- **Memory Management**: Enhanced resource lifecycle management with explicit creation/destruction
- **Error Handling**: Improved error paths and debugging capabilities

**üß™ Validation Approach:**
- **Comprehensive Testing**: Automated comparison with reference implementations
- **Bit-Perfect Verification**: Zero-tolerance validation ensuring computational accuracy
- **Comprehensive Scenarios**: 8 test configurations covering micro to memory-intensive workloads
- **Docker Isolation**: Containerized testing ensuring consistent validation environments

**üìä Migration Results:**
- ‚úÖ **Perfect Accuracy**: 0.00e+00 difference across all test configurations
- ‚úÖ **Zero Performance Regression**: Identical throughput and memory utilization
- ‚úÖ **Full API Compatibility**: Drop-in replacement for existing deployments
- ‚úÖ **Future-Proof Architecture**: Ready for CUDA 13.x+ and upcoming GPU generations

For complete technical details, see [`CUDA_MIGRATION_COMPARISON.md`](CUDA_MIGRATION_COMPARISON.md).

### Docker-Based Testing Infrastructure

The repository includes production-ready testing infrastructure:

**üê≥ Testing Workflows:**

1. **Local Development Testing:**
   ```bash
   ./test-runner.sh basic          # Quick functionality verification
   ```

2. **Validation Testing:**
   ```bash
   ./test-runner.sh validation      # Comprehensive accuracy verification
   ```

3. **Advanced Testing:**
   ```bash
   cd test
   ./cross-version-test.sh --comprehensive  # All test configurations
   ./cross-version-test.sh -c texture_stress # Specific scenario testing
   ```

**üìã Test Configurations:**

| Configuration | Stations | Frequencies | Samples | Focus Area |
|---------------|----------|-------------|---------|------------|
| micro | 64 | 3 | 256 | Quick validation |
| texture_stress | 256 | 30 | 2048 | Texture memory performance |
| memory_intensive | 384 | 15 | 1536 | Memory management |
| large | 512 | 20 | 2048 | Scalability testing |

**üîç Testing Documentation:**
- [`test/README.md`](test/README.md) - Testing framework overview
- [`test/CROSS_VERSION_TESTING.md`](test/CROSS_VERSION_TESTING.md) - Detailed validation procedures
- [`test/COMPREHENSIVE_TESTING_SUMMARY.md`](test/COMPREHENSIVE_TESTING_SUMMARY.md) - Enhanced testing capabilities

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

### Development Guidelines
- Maintain computational accuracy and performance characteristics
- Include comprehensive testing for any changes
- Follow existing coding patterns and documentation standards
- Validate changes against CUDA 12.x environments

## Support and Issues

- **Issues**: Report bugs or request features via [GitHub Issues](https://github.com/jbmorgado/xGPU-future/issues)
- **Documentation**: Comprehensive guides available in the [`test/`](test/) directory
- **Examples**: See [`src/cuda_correlator.c`](src/cuda_correlator.c) for usage patterns

---

**[‚¨Ü Back to Top](#xgpu-future-cuda-12x-compatible-correlator-library)**
